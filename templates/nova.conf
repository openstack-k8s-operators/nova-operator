[DEFAULT]
{{/*controller services will have stable names provide by a stateful set*/}}
{{ if eq .service_name "nova-compute"}}
host={{ .fqdn }}
{{/*the current default in ooo is wrong we should limit to 1*/}}
# concurrent live migrations are more likely to fail and are slower
# overall then serializing live migrations so set this to 1 explictly
max_concurrent_live_migrations=1
{{ if .enable_ceilometer }}
instance_usage_audit = true
instance_usage_audit_period = hour
{{end}}
{{end}}
state_path = /var/lib/nova
allow_resize_to_same_host = true
{{/*we can do log rotation differntly later but for now lets limit by size*/}}
# enable log rotation in oslo config by default
max_logfile_count=5
max_logfile_size_mb=50
log_rotation_type=size
{{if .log_file }}
log_file = {{ .log_file }}
{{end}}
debug=true
# ensure safe defaults for new hosts
initial_cpu_allocation_ratio=4.0
initial_ram_allocation_ratio=1.0
initial_disk_allocation_ratio=0.9
{{/*we really should change this upstream too*/}}
# use predictable instnace names for simpler debuging.
instance_name_template="instance-%(uuid)s"
{{/*using a config drive will void issues with ovn and metadata*/}}
force_config_drive=True
{{if .transport_url}}transport_url={{.transport_url}}{{end}}

{{if eq .service_name "nova-api"}}
# scaling should be done by running more pods
osapi_compute_workers=1
metadata_workers=1
{{else if eq .service_name "nova-metadata"}}
# scaling should be done by running more pods
osapi_compute_workers=1
metadata_workers=1
{{end}}

[oslo_concurrency]
lock_path = /var/lib/nova/tmp

[oslo_messaging_rabbit]
amqp_durable_queues=false
amqp_auto_delete=false
# we should consider using quorum queues instead
# rabbit_quorum_queue=true
{{/*we might just want to make this always false*/}}
{{ if eq .service_name "nova-api"}}
heartbeat_in_pthread=true
{{else}}
heartbeat_in_pthread=false
{{end}}

{{ if eq .service_name "nova-api"}}
[oslo_policy]
enforce_new_defaults=false
{{end}}

{{ if eq .service_name "nova-conductor"}}
[conductor]
# scaling should be done by running more pods
workers=1
{{end}}

{{ if eq .service_name "nova-scheduler"}}
[filter_scheduler]
available_filters = nova.scheduler.filters.all_filters
enabled_filters = ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter,SameHostFilter,DifferentHostFilter,PciPassthroughFilter,NUMATopologyFilter
# until we can disabel upcalls we can't turn this off by default
# track_instance_changes = false
shuffle_best_same_weighed_hosts = true

[scheduler]
max_attempts = 10
{{/*We should turn this off but for now 60 will work*/}}
discover_hosts_in_cells_interval = 60
# scaling should be done by running more pods
workers = 1
limit_tenants_to_placement_aggregate=true
placement_aggregate_required_for_tenants=false
query_placement_for_routed_network_aggregates=true
query_placement_for_availability_zone=true
query_placement_for_image_type_support=true
enable_isolated_aggregate_filtering=true
image_metadata_prefilter=true
{{end}}

{{if eq .service_name "nova-novncproxy"}}
[console]
ssl_minimum_version=tlsv1_3
{{end}}

{{if eq .service_name "nova-api" "nova-metadata-api"}}
[api]
use_forwarded_for = true
auth_strategy = keystone
[oslo_middleware]
enable_proxy_headers_parsing = True
[wsgi]
api_paste_config = /etc/nova/api-paste.ini
{{end}}

[upgrade_levels]
compute = auto

[oslo_messaging_notifications]
{{ if .nova_enabled_notification }}
transport_url =  {{ .nova_cell_notify_transport_url }}
driver = messagingv2
notification_format=versioned
{{ else }}
driver = noop
{{end}}

[notifications]
{{if .enable_ceilometer }}
notify_on_state_change = vm_and_task_state
{{ end }}

{{ if eq .service_name "nova-vnc-proxy"}}
{{/*This is need for the novnc proxy but also api?*/}}
[vnc]
novncproxy_host = {{ .novncproxy_service_host }}
novncproxy_port = {{ .nova_novncproxy_listen_port }}
server_listen = {{ .api_interface_address }}
server_proxyclient_address = {{ .api_interface_address }}
novncproxy_base_url = {{ .public_protocol }}://{{ .novncproxy_service_host }}/vnc_lite.html
{{end}}

[cache]
# always enable caching
enabled = True
{{if .memcache_servers}}
# on contoler we prefer to use memcache when its deployed
backend = oslo_cache.memcache_pool
memcache_servers = {{range .memcache_servers -}}{{.address}}:{{.port}},{{- end}}
{{else}}
# on compute nodes or where memcache is not deployed we should use an in memory
# dict cache
backend = oslo_cache.dict
{{end}}

[workarounds]
disable_fallback_pcpu_query=true
enable_qemu_monitor_announce_self=true
reserve_disk_resource_for_image_cache=true
{{/* we may want to enable some fo these workarounds by default*/}}
# never_download_image_if_on_rbd=true
# disable_group_policy_check_upcall=true
# wait_for_vif_plugged_event_during_hard_reboot=["normal"]
# we might want to enable these for upgrades.
# disable_compute_service_check_for_ffu=true
# skip_cpu_compare_on_dest=true
# skip_hypervisor_version_check_on_lm=true

{{ if eq .service_name "nova-compute"}}
[os_vif_ovs]
ovsdb_interface=vsctl

[libvirt]
live_migration_permit_post_copy=true
live_migration_permit_auto_converge=true
live_migration_timeout_action=force_complete
cpu_mode=host-model
hw_machine_type=x86_64=q35
sysinfo_serial=unique
mem_stats_period_seconds=0
num_pcie_ports=24
images_type=qcow2
hw_disk_discard=unmap
rx_queue_size=512
tx_queue_size=512
swtpm_enabled=True
{{end}}


{{/* we could check the service_name but instead we just include the db section if we have the db address*/}}
{{if .cell_db_address}}
[database]
connection = mysql+pymysql://{{ .cell_db_user }}:{{ .cell_db_password}}@{{ .cell_db_address }}/{{ .cell_db_name }}
{{end}}


{{if .api_db_address}}
[api_database]
connection = mysql+pymysql://{{ .api_db_user }}:{{ .api_db_password }}@{{ .api_db_address }}/{{ .api_db_name }}
{{end}}

[keystone_authtoken]
www_authenticate_uri = {{ .keystone_internal_url }}
auth_url =  {{ .keystone_internal_url }}
auth_type = password
project_domain_name = {{ .default_project_domain }}
user_domain_name = {{ .default_user_domain}}
project_name = service
username = {{ .nova_keystone_user }}
password = {{ .nova_keystone_password }}
cafile = {{ .openstack_cacert }}
region_name = {{ .openstack_region_name }}

[placement]
www_authenticate_uri = {{ .keystone_internal_url }}
auth_url =  {{ .keystone_internal_url }}
auth_type = password
project_domain_name = {{ .default_project_domain }}
user_domain_name = {{ .default_user_domain}}
project_name = service
username = {{ .nova_keystone_user }}
password = {{ .nova_keystone_password }}
cafile = {{ .openstack_cacert }}
region_name = {{ .openstack_region_name }}
valid_interfaces = internal

[glance]
www_authenticate_uri = {{ .keystone_internal_url }}
auth_url = {{ .keystone_internal_url }}
auth_type = password
project_domain_name = {{ .default_project_domain }}
user_domain_name = {{ .default_user_domain}}
project_name = service
username = {{ .nova_keystone_user }}
password = {{ .nova_keystone_password }}
cafile = {{ .openstack_cacert }}
region_name = {{ .openstack_region_name }}
valid_interfaces = internal
{{if .debug }}debug=true{{end}}

[neutron]
www_authenticate_uri = {{ .keystone_internal_url }}
auth_url = {{ .keystone_internal_url }}
auth_type = password
project_domain_name = {{ .default_project_domain }}
user_domain_name = {{ .default_user_domain}}
project_name = service
username = {{ .nova_keystone_user }}
password = {{ .nova_keystone_password }}
cafile = {{ .openstack_cacert }}
region_name = {{ .openstack_region_name }}
valid_interfaces = internal
metadata_proxy_shared_secret = {{ .metadata_secret }}
service_metadata_proxy = true

{{if .enable_cinder }}
[cinder]
www_authenticate_uri = {{ .keystone_internal_url }}
auth_url = {{ .keystone_internal_url }}
auth_type = password
project_domain_name = {{ .default_project_domain }}
user_domain_name = {{ .default_user_domain}}
project_name = service
username = {{ .nova_keystone_user }}
password = {{ .nova_keystone_password }}
cafile = {{ .openstack_cacert }}
region_name = {{ .openstack_region_name }}
valid_interfaces = internal
{{end}}
